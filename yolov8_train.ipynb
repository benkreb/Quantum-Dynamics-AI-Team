{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics \n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install roboflow\n",
    "%pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"dG6FFG86MVKR54GL8BVl\")\n",
    "project = rf.workspace(\"datasetteknofest\").project(\"sa-59vao\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {dataset.location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"Desktop/AI/yolov8/New folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8m.yaml')  # build a new model from YAML\n",
    "model = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)\n",
    "model = YOLO('yolov8m.yaml').load('yolov8m.pt')  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='data.yaml', epochs=100, imgsz=640,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from ultralytics.utils.checks import check_imshow\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "model = YOLO(\"runs/detect/train3/weights/last.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "video_path = \"video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "result = cv2.VideoWriter(\"object_tracking3.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "\n",
    "            # Extract prediction results\n",
    "            clss = results[0].boxes.cls.cpu().tolist()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            confs = results[0].boxes.conf.float().cpu().tolist()\n",
    "\n",
    "            # Annotator Init\n",
    "            annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "            for box, cls, track_id in zip(boxes, clss, track_ids):\n",
    "                annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "                # Store tracking history\n",
    "                track = track_history[track_id]\n",
    "                track.append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                if len(track) > 30:\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Plot tracks\n",
    "                points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                cv2.circle(frame, (track[-1]), 7, colors(int(cls), True), -1)\n",
    "                cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=2)\n",
    "\n",
    "        result.write(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "result.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"is available\")\n",
    "else:\n",
    "    print(\"Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='VisDrone.yaml', epochs=200, imgsz=640, device=0,save_json=True,name=\"optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('runs/detect/train7/weights/best.pt')  # Load a custom trained model\n",
    "\n",
    "# Perform tracking with the model\n",
    "#results = model.track(source=\"video.mp4\", show=True)  # Tracking with default tracker\n",
    "results = model.track(source=\"video.mp4\", show=True, tracker=\"bytetrack.yaml\")  # Tracking with ByteTrack tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ultralytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
